{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_vocab for yelp_review sentiment classification dataset\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I got 'new' tires from them and within two wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't waste your time.  We had two different p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>All I can say is the worst! We were the only 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I have been to this restaurant twice and was d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Food was NOT GOOD at all! My husband &amp; I ate h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  1  I got 'new' tires from them and within two wee...\n",
       "1  1  Don't waste your time.  We had two different p...\n",
       "2  1  All I can say is the worst! We were the only 2...\n",
       "3  1  I have been to this restaurant twice and was d...\n",
       "4  1  Food was NOT GOOD at all! My husband & I ate h..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yahoo_data = pd.read_csv('dataset/yelp_review_small/test.csv', header=None)\n",
    "Yahoo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data...\n",
      "Number of sentences is 650000\n",
      "Max Length of sentences is 1228\n",
      "\n",
      "Creating test data...\n",
      "Number of sentences is 50000\n",
      "Max Length of sentences is 1157\n",
      "Train / Test file created\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Train and Test Files for the yelp_review sentiment classification experiment\n",
    "\"\"\"\n",
    "trainFile = 'dataset/yelp_review_small/train.csv'\n",
    "testFile = 'dataset/yelp_review_small/test.csv'\n",
    "\n",
    "# # load standford POS tagger\n",
    "# path_to_model_pos = \"/data2/Stanford_tools/stanford-postagger-2018-02-27/models/english-bidirectional-distsim.tagger\"\n",
    "# path_to_jar_pos = \"/data2/Stanford_tools/stanford-postagger-2018-02-27/stanford-postagger.jar\"\n",
    "# pos_tagger=StanfordPOSTagger(path_to_model_pos, path_to_jar_pos)\n",
    "# # pos_tagger.java_options='-mx4096m'          ### Setting higher memory limit for long sentences\n",
    "\n",
    "\n",
    "# load standford NER tagger\n",
    "# path_to_model_ner = '/data2/Stanford_tools/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "# path_to_jar_ner = '/data2/Stanford_tools/stanford-ner-2017-06-09/stanford-ner.jar'\n",
    "# ner_tagger = StanfordNERTagger(path_to_model_ner,path_to_jar_ner,  encoding='utf-8')\n",
    "\n",
    "def createFile(filepath, outputpath):\n",
    "    Yahoo_data = pd.read_csv(filepath, header=None)\n",
    "    labels = Yahoo_data.iloc[:,0].tolist()\n",
    "    samples = Yahoo_data.iloc[:,1].tolist()  \n",
    "    \n",
    "    fOut = open(outputpath, 'w')\n",
    "    data_json = []\n",
    "\n",
    "    maxLen_Sentence = 0\n",
    "    num_Sentence = 0\n",
    "    \n",
    "    def preprocess_doc(document):\n",
    "        document = document.replace(u'\\\\n',u'\\n')\n",
    "        document = document.replace(u'\\\\r',u'\\r')\n",
    "        document = document.replace(u'\\\\\"',u'\\\"')\n",
    "        document = document.replace(\"<PAD>\", \" _PAD_ \")\n",
    "        return document\n",
    "\n",
    "    for idx in range(0, len(samples)):\n",
    "        num_Sentence += 1\n",
    "        \n",
    "        data_dict = {}\n",
    "        label = labels[idx]\n",
    "        sentence = preprocess_doc(samples[idx])\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "        data_dict['label'] = label\n",
    "        data_dict['token'] = tokens\n",
    "        \n",
    "#         # NER tagging\n",
    "#         tokens_ner = ner_tagger.tag(tokens)\n",
    "        # POS tagging\n",
    "#         tokens_pos = pos_tagger.tag(tokens)\n",
    "        \n",
    "#         data_dict['stanford_pos'] = [pos[1] for pos in tokens_pos]\n",
    "#         data_dict['stanford_ner'] = [ner[1] for ner in tokens_ner]\n",
    "        \n",
    "        if len(tokens) > maxLen_Sentence:\n",
    "            maxLen_Sentence = len(tokens)\n",
    "\n",
    "        data_json.append(data_dict)\n",
    "        \n",
    "    with open(outputpath, 'w') as outfile:  \n",
    "        json.dump(data_json, outfile)\n",
    "    print(\"Number of sentences is\", num_Sentence)\n",
    "    print(\"Max Length of sentences is\", maxLen_Sentence)\n",
    "    \n",
    "\n",
    "print(\"Creating training data...\")\n",
    "createFile(trainFile, \"dataset/yelp_review_small/train_processed.json\")\n",
    "print(\"\\nCreating test data...\")\n",
    "createFile(testFile, \"dataset/yelp_review_small/test_processed.json\")\n",
    "print(\"Train / Test file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare vocabulary and initial word vectors.\n",
    "\"\"\"\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from utils import vocab, constant, helper\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Prepare vocab for text classification.')\n",
    "    parser.add_argument('--data_dir', default='dataset/yelp_review_small', help='TACRED directory.')\n",
    "    parser.add_argument('--vocab_dir', default='dataset/yelp_review_small', help='Output vocab directory.')\n",
    "    parser.add_argument('--glove_dir', default='dataset', help='GloVe directory.')\n",
    "    parser.add_argument('--wv_file', default='glove.840B.300d.txt', help='GloVe vector file.')\n",
    "    parser.add_argument('--wv_dim', type=int, default=300, help='GloVe vector dimension.')\n",
    "    parser.add_argument('--min_freq', type=int, default=0, help='If > 0, use min_freq as the cutoff.')\n",
    "    parser.add_argument('--lower', action='store_true', help='If specified, lowercase all words.')\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "    \n",
    "\n",
    "def load_tokens(filename):\n",
    "    with open(filename) as infile:\n",
    "        data = json.load(infile)\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            tokens += d['token']\n",
    "    print(\"{} tokens from {} examples loaded from {}.\".format(len(tokens), len(data), filename))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_vocab(tokens, glove_vocab, min_freq):\n",
    "    \"\"\" build vocab from tokens and glove words. \"\"\"\n",
    "    counter = Counter(t for t in tokens if t != '_PAD_')\n",
    "    # if min_freq > 0, use min_freq, otherwise keep all glove words\n",
    "    if min_freq > 0:\n",
    "        v = sorted([t for t in counter if counter.get(t) >= min_freq], key=counter.get, reverse=True)\n",
    "    else:\n",
    "        v = sorted([t for t in counter if t in glove_vocab], key=counter.get, reverse=True)\n",
    "    # add special tokens and entity mask tokens\n",
    "    v = constant.VOCAB_PREFIX + v\n",
    "    print(\"vocab built with {}/{} words.\".format(len(v), len(counter)))\n",
    "    return v\n",
    "\n",
    "\n",
    "def count_oov(tokens, vocab):\n",
    "    c = Counter(t for t in tokens)\n",
    "    total = sum(c.values())\n",
    "    matched = sum(c[t] for t in vocab)\n",
    "    return total, total-matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_dir='dataset/yelp_review_small', glove_dir='/data2/pengfei_data/data', lower=False, min_freq=0, vocab_dir='dataset/yelp_review_small', wv_dim=300, wv_file='glove.840B.300d.txt')\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "args.lower = False\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "train_file = args.data_dir + '/train_processed.json'\n",
    "# dev_file = args.data_dir + '/dev.json'\n",
    "test_file = args.data_dir + '/test_processed.json'\n",
    "wv_file = args.glove_dir + '/' + args.wv_file\n",
    "wv_dim = args.wv_dim\n",
    "\n",
    "# output files\n",
    "helper.ensure_dir(args.vocab_dir)\n",
    "vocab_file = args.vocab_dir + '/vocab.pkl'\n",
    "emb_file = args.vocab_dir + '/embedding.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files...\n",
      "101170944 tokens from 650000 examples loaded from dataset/yelp_review_small/train_processed.json.\n",
      "7795550 tokens from 50000 examples loaded from dataset/yelp_review_small/test_processed.json.\n",
      "loading glove...\n",
      "2195892 words loaded from glove.\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "print(\"loading files...\")\n",
    "train_tokens = load_tokens(train_file)\n",
    "# dev_tokens = load_tokens(dev_file)\n",
    "test_tokens = load_tokens(test_file)\n",
    "if args.lower:\n",
    "    train_tokens, test_tokens = [[t.lower() for t in tokens] for tokens in (train_tokens, test_tokens)]\n",
    "    \n",
    "# load glove\n",
    "print(\"loading glove...\")\n",
    "glove_vocab = vocab.load_glove_vocab(wv_file, wv_dim)\n",
    "print(\"{} words loaded from glove.\".format(len(glove_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n",
      "vocab built with 230902/508651 words.\n",
      "calculating oov...\n",
      "train oov: 594450/101170944 (0.59%)\n",
      "test oov: 52038/7795550 (0.67%)\n"
     ]
    }
   ],
   "source": [
    "print(\"building vocab...\")\n",
    "v = build_vocab(train_tokens, glove_vocab, args.min_freq)\n",
    "\n",
    "print(\"calculating oov...\")\n",
    "datasets = {'train': train_tokens, 'test': test_tokens}\n",
    "for dname, d in datasets.items():\n",
    "    total, oov = count_oov(d, v)\n",
    "    print(\"{} oov: {}/{} ({:.2f}%)\".format(dname, oov, total, oov*100.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building embeddings...\n",
      "embedding size: 230902 x 300\n",
      "dumping to files...\n",
      "all done.\n"
     ]
    }
   ],
   "source": [
    "print(\"building embeddings...\")\n",
    "embedding = vocab.build_embedding(wv_file, v, wv_dim)\n",
    "print(\"embedding size: {} x {}\".format(*embedding.shape))\n",
    "\n",
    "print(\"dumping to files...\")\n",
    "with open(vocab_file, 'wb') as outfile:\n",
    "    pickle.dump(v, outfile)\n",
    "np.save(emb_file, embedding)\n",
    "print(\"all done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
